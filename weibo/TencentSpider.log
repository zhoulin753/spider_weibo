2018-10-22 08:57:31 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 08:57:31 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 08:57:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 08:57:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 08:57:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 08:57:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 08:57:31 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 08:57:31 [scrapy.core.engine] INFO: Spider opened
2018-10-22 08:57:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 08:57:31 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 09:00:39 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 13 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:01:39 [scrapy.extensions.logstats] INFO: Crawled 16 pages (at 3 pages/min), scraped 8 items (at 8 items/min)
2018-10-22 09:02:44 [scrapy.extensions.logstats] INFO: Crawled 20 pages (at 4 pages/min), scraped 8 items (at 0 items/min)
2018-10-22 09:03:18 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 09:03:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 771544,
 'downloader/response_count': 22,
 'downloader/response_status_count/200': 22,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 1, 3, 18, 359565),
 'item_scraped_count': 8,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 22,
 'scheduler/dequeued': 22,
 'scheduler/dequeued/memory': 22,
 'scheduler/enqueued': 22,
 'scheduler/enqueued/memory': 22,
 'start_time': datetime.datetime(2018, 10, 22, 0, 57, 31, 623560)}
2018-10-22 09:03:18 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 09:07:34 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 09:07:34 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 09:07:34 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 09:07:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 09:07:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 09:07:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 09:07:35 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 09:07:35 [scrapy.core.engine] INFO: Spider opened
2018-10-22 09:07:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:07:35 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 09:08:40 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:09:36 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:10:42 [scrapy.extensions.logstats] INFO: Crawled 12 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:11:14 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 09:11:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 700396,
 'downloader/response_count': 14,
 'downloader/response_status_count/200': 14,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 1, 11, 14, 417503),
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 14,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2018, 10, 22, 1, 7, 35, 66934)}
2018-10-22 09:11:14 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 09:13:03 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 09:13:03 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 09:13:03 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 09:13:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 09:13:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 09:13:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 09:13:04 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 09:13:04 [scrapy.core.engine] INFO: Spider opened
2018-10-22 09:13:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:13:04 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 09:14:30 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:16:35 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 09:16:35 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 09:16:35 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 09:16:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 09:16:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 09:16:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 09:16:35 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 09:16:35 [scrapy.core.engine] INFO: Spider opened
2018-10-22 09:16:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:16:35 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 09:17:44 [scrapy.extensions.logstats] INFO: Crawled 7 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:18:45 [scrapy.extensions.logstats] INFO: Crawled 12 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:19:10 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 09:19:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 346171,
 'downloader/response_count': 14,
 'downloader/response_status_count/200': 14,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 1, 19, 10, 774908),
 'log_count/INFO': 9,
 'log_count/WARNING': 1,
 'response_received_count': 14,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2018, 10, 22, 1, 16, 35, 551709)}
2018-10-22 09:19:10 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 09:20:01 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 09:20:01 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 09:20:01 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 09:20:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 09:20:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 09:20:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 09:20:01 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 09:20:01 [scrapy.core.engine] INFO: Spider opened
2018-10-22 09:20:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:20:01 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 09:21:03 [scrapy.extensions.logstats] INFO: Crawled 7 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:22:05 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:22:05 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 09:22:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 342141,
 'downloader/response_count': 14,
 'downloader/response_status_count/200': 14,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 1, 22, 5, 691227),
 'log_count/INFO': 9,
 'log_count/WARNING': 1,
 'response_received_count': 14,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2018, 10, 22, 1, 20, 1, 845091)}
2018-10-22 09:22:05 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 09:31:16 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 09:31:16 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 09:31:16 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 09:31:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 09:31:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 09:31:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 09:31:17 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 09:31:17 [scrapy.core.engine] INFO: Spider opened
2018-10-22 09:31:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:31:17 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 09:32:25 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 8 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:33:17 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:33:17 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 09:33:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 346172,
 'downloader/response_count': 14,
 'downloader/response_status_count/200': 14,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 1, 33, 17, 484669),
 'log_count/INFO': 9,
 'log_count/WARNING': 1,
 'response_received_count': 14,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2018, 10, 22, 1, 31, 17, 117794)}
2018-10-22 09:33:17 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 09:46:42 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 09:46:42 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 09:46:42 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 09:46:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-22 09:46:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 09:46:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 09:46:42 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 09:51:31 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 09:51:31 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 09:51:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 09:51:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-22 09:51:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 09:51:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 09:51:31 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 09:51:31 [scrapy.core.engine] INFO: Spider opened
2018-10-22 09:51:32 [py.warnings] WARNING: c:\users\admin\pycharmprojects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 09:54:27 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 09:54:27 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 09:54:27 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 09:54:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 09:54:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 09:54:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 09:54:27 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 09:54:27 [scrapy.core.engine] INFO: Spider opened
2018-10-22 09:54:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:54:27 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 09:55:28 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:56:32 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:56:48 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 09:56:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 346003,
 'downloader/response_count': 14,
 'downloader/response_status_count/200': 14,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 1, 56, 48, 213282),
 'log_count/INFO': 9,
 'log_count/WARNING': 1,
 'response_received_count': 14,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2018, 10, 22, 1, 54, 27, 546806)}
2018-10-22 09:56:48 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 10:35:51 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 10:35:51 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 10:35:51 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 10:35:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 10:35:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 10:35:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 10:35:51 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 10:35:51 [scrapy.core.engine] INFO: Spider opened
2018-10-22 10:35:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 10:35:51 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 10:37:01 [scrapy.extensions.logstats] INFO: Crawled 7 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 10:38:00 [scrapy.extensions.logstats] INFO: Crawled 12 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 10:38:24 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 10:38:24 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 346354,
 'downloader/response_count': 14,
 'downloader/response_status_count/200': 14,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 2, 38, 24, 39269),
 'log_count/INFO': 9,
 'log_count/WARNING': 1,
 'response_received_count': 14,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2018, 10, 22, 2, 35, 51, 942058)}
2018-10-22 10:38:24 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 10:38:31 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 10:38:31 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 10:38:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 10:38:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 10:38:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 10:38:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 10:38:32 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 10:38:32 [scrapy.core.engine] INFO: Spider opened
2018-10-22 10:38:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 10:38:32 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 10:39:34 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 10:40:33 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 10:40:41 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 10:40:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 346354,
 'downloader/response_count': 14,
 'downloader/response_status_count/200': 14,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 2, 40, 41, 892971),
 'log_count/INFO': 9,
 'log_count/WARNING': 1,
 'response_received_count': 14,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2018, 10, 22, 2, 38, 32, 298371)}
2018-10-22 10:40:41 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 10:41:25 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 10:41:25 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 10:41:25 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 10:41:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 10:41:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 10:41:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 10:41:26 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 10:41:26 [scrapy.core.engine] INFO: Spider opened
2018-10-22 10:41:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 10:41:26 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 10:42:30 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 10:43:36 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 10:43:44 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 10:43:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 346354,
 'downloader/response_count': 14,
 'downloader/response_status_count/200': 14,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 2, 43, 44, 533552),
 'log_count/INFO': 9,
 'log_count/WARNING': 1,
 'response_received_count': 14,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2018, 10, 22, 2, 41, 26, 225994)}
2018-10-22 10:43:44 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 14:47:24 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 14:47:24 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 14:47:24 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 14:47:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 14:47:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 14:47:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 14:47:26 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 14:47:26 [scrapy.core.engine] INFO: Spider opened
2018-10-22 14:47:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 14:47:27 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 14:48:28 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 14:48:53 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 14:48:53 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 14:48:53 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 14:48:53 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 14:48:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 14:48:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 14:48:54 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 14:48:54 [scrapy.core.engine] INFO: Spider opened
2018-10-22 14:48:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 14:48:54 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 14:49:25 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 14:49:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 26322,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 6, 49, 25, 392434),
 'log_count/INFO': 7,
 'log_count/WARNING': 1,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2018, 10, 22, 6, 48, 54, 217364)}
2018-10-22 14:49:25 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 14:49:47 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 14:49:47 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 14:49:47 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 14:49:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 14:49:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 14:49:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 14:49:48 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 14:49:48 [scrapy.core.engine] INFO: Spider opened
2018-10-22 14:49:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 14:49:48 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 14:50:34 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 14:50:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 386397,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 6, 50, 34, 81059),
 'log_count/INFO': 7,
 'log_count/WARNING': 1,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2018, 10, 22, 6, 49, 48, 222578)}
2018-10-22 14:50:34 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 14:52:08 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 14:52:08 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 14:52:08 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 14:52:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 14:52:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 14:52:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 14:52:09 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 14:52:09 [scrapy.core.engine] INFO: Spider opened
2018-10-22 14:52:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 14:52:09 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 14:53:16 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 14:53:16 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 14:53:16 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 14:53:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 14:53:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 14:53:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 14:53:16 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 14:53:16 [scrapy.core.engine] INFO: Spider opened
2018-10-22 14:53:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 14:53:16 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 14:54:31 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 14:56:20 [scrapy.extensions.logstats] INFO: Crawled 15 pages (at 9 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 14:57:21 [scrapy.extensions.logstats] INFO: Crawled 20 pages (at 5 pages/min), scraped 8 items (at 8 items/min)
2018-10-22 14:57:46 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 14:57:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 706599,
 'downloader/response_count': 22,
 'downloader/response_status_count/200': 22,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 6, 57, 46, 775237),
 'item_scraped_count': 8,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 22,
 'scheduler/dequeued': 22,
 'scheduler/dequeued/memory': 22,
 'scheduler/enqueued': 22,
 'scheduler/enqueued/memory': 22,
 'start_time': datetime.datetime(2018, 10, 22, 6, 53, 16, 535263)}
2018-10-22 14:57:46 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 15:04:16 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 15:04:16 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 15:04:16 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 15:04:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-22 15:04:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 15:04:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 15:04:17 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 15:04:17 [scrapy.core.engine] INFO: Spider opened
2018-10-22 15:04:17 [py.warnings] WARNING: c:\users\admin\pycharmprojects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-23 11:30:17 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-23 11:30:17 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-23 11:30:17 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-23 11:30:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-23 11:30:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-23 11:30:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-23 11:30:19 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-23 11:30:47 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-23 11:30:47 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-23 11:30:47 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-23 11:30:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-23 11:30:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-23 11:30:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-23 11:30:47 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-23 11:30:47 [scrapy.core.engine] INFO: Spider opened
2018-10-23 11:30:47 [py.warnings] WARNING: c:\users\admin\pycharmprojects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-23 11:40:37 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-23 11:40:37 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-23 11:40:37 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-23 11:40:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-23 11:40:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-23 11:40:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-23 11:40:38 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-23 11:41:32 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-23 11:41:32 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-23 11:41:32 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-23 11:41:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-23 11:41:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-23 11:41:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-23 11:41:33 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-23 11:42:43 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-23 11:42:43 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-23 11:42:43 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-23 11:42:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-23 11:42:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-23 11:42:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-23 11:42:44 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-23 11:42:44 [scrapy.core.engine] INFO: Spider opened
2018-10-23 11:42:44 [py.warnings] WARNING: c:\users\admin\pycharmprojects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-23 19:12:18 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-23 19:12:18 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-23 19:12:18 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-23 19:12:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-23 19:12:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-23 19:12:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-23 19:12:20 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-23 19:12:20 [scrapy.core.engine] INFO: Spider opened
2018-10-23 19:12:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-23 19:12:20 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-23 19:13:44 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2018-10-23 19:16:02 [scrapy.extensions.logstats] INFO: Crawled 15 pages (at 9 pages/min), scraped 0 items (at 0 items/min)
2018-10-23 19:16:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298337355482343> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 41, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 50, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-23 19:16:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298268774388807> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 41, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 50, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-23 19:16:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298240257305277> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 41, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 50, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-23 19:16:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298328660647409> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 41, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 50, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-23 19:16:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298303071242458> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 41, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 50, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-23 19:16:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298245621821033> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 41, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 50, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-23 19:16:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298286046532123> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 41, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 50, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-23 19:16:21 [scrapy.extensions.logstats] INFO: Crawled 16 pages (at 1 pages/min), scraped 1 items (at 1 items/min)
2018-10-23 19:17:26 [scrapy.extensions.logstats] INFO: Crawled 21 pages (at 5 pages/min), scraped 1 items (at 0 items/min)
2018-10-23 19:17:40 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-23 19:17:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 549468,
 'downloader/response_count': 22,
 'downloader/response_status_count/200': 22,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 23, 11, 17, 40, 639068),
 'item_scraped_count': 1,
 'log_count/ERROR': 7,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 22,
 'scheduler/dequeued': 22,
 'scheduler/dequeued/memory': 22,
 'scheduler/enqueued': 22,
 'scheduler/enqueued/memory': 22,
 'spider_exceptions/IndexError': 7,
 'start_time': datetime.datetime(2018, 10, 23, 11, 12, 20, 568998)}
2018-10-23 19:17:40 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-24 10:01:07 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-24 10:01:07 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-24 10:01:07 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-24 10:01:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-24 10:01:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-24 10:01:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-24 10:01:09 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-24 10:01:09 [scrapy.core.engine] INFO: Spider opened
2018-10-24 10:01:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-24 10:01:09 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-24 10:04:06 [scrapy.extensions.logstats] INFO: Crawled 15 pages (at 15 pages/min), scraped 0 items (at 0 items/min)
2018-10-24 10:07:35 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-24 10:07:35 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-24 10:07:35 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-24 10:07:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-24 10:07:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-24 10:07:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-24 10:07:36 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-24 10:07:36 [scrapy.core.engine] INFO: Spider opened
2018-10-24 10:07:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-24 10:07:36 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-24 10:10:33 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 13 pages/min), scraped 0 items (at 0 items/min)
2018-10-24 10:10:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298576351129119> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 41, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 10:10:49 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2018-10-24 10:11:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298579383609650> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 41, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 10:11:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298576472766885> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 41, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 10:11:50 [scrapy.extensions.logstats] INFO: Crawled 18 pages (at 4 pages/min), scraped 5 items (at 5 items/min)
2018-10-24 10:13:03 [scrapy.extensions.logstats] INFO: Crawled 21 pages (at 3 pages/min), scraped 5 items (at 0 items/min)
2018-10-24 10:13:18 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-24 10:13:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 1049750,
 'downloader/response_count': 22,
 'downloader/response_status_count/200': 22,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 24, 2, 13, 18, 61350),
 'item_scraped_count': 5,
 'log_count/ERROR': 3,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 22,
 'scheduler/dequeued': 22,
 'scheduler/dequeued/memory': 22,
 'scheduler/enqueued': 22,
 'scheduler/enqueued/memory': 22,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2018, 10, 24, 2, 7, 36, 770469)}
2018-10-24 10:13:18 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-24 10:37:42 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-24 10:37:42 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-24 10:37:42 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-24 10:37:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-24 10:37:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-24 10:37:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-24 10:37:44 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-24 10:37:44 [scrapy.core.engine] INFO: Spider opened
2018-10-24 10:37:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-24 10:37:44 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-24 10:40:08 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 13 pages/min), scraped 0 items (at 0 items/min)
2018-10-24 10:40:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298589714154908> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 46, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 10:42:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298584286727740> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 46, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 10:42:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298587466038239> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 46, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 10:42:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298589395396497> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 46, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 10:42:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298587268898129> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 46, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 10:42:06 [scrapy.extensions.logstats] INFO: Crawled 22 pages (at 9 pages/min), scraped 3 items (at 3 items/min)
2018-10-24 10:42:23 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/ttarticle/p/show?id=2309404298581426215461>
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\urllib3\connectionpool.py", line 384, in _make_request
    six.raise_from(e, None)
  File "<string>", line 2, in raise_from
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\urllib3\connectionpool.py", line 380, in _make_request
    httplib_response = conn.getresponse()
  File "C:\Python36\lib\http\client.py", line 1331, in getresponse
    response.begin()
  File "C:\Python36\lib\http\client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "C:\Python36\lib\http\client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "C:\Python36\lib\socket.py", line 586, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\middlewares\middleware.py", line 19, in process_request
    driver.get(request.url)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 319, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\remote\remote_connection.py", line 376, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\remote\remote_connection.py", line 404, in _request
    resp = http.request(method, url, body=body, headers=headers)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\urllib3\request.py", line 72, in request
    **urlopen_kw)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\urllib3\request.py", line 150, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\urllib3\poolmanager.py", line 322, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\urllib3\util\retry.py", line 367, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\urllib3\packages\six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\urllib3\connectionpool.py", line 384, in _make_request
    six.raise_from(e, None)
  File "<string>", line 2, in raise_from
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\urllib3\connectionpool.py", line 380, in _make_request
    httplib_response = conn.getresponse()
  File "C:\Python36\lib\http\client.py", line 1331, in getresponse
    response.begin()
  File "C:\Python36\lib\http\client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "C:\Python36\lib\http\client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "C:\Python36\lib\socket.py", line 586, in readinto
    return self._sock.recv_into(b)
urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None))
2018-10-24 10:42:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298589458312259> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 46, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 10:43:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298582751581461> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 46, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 10:43:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298575797469703> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 46, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 10:43:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298576997002546> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 46, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 10:43:06 [scrapy.extensions.logstats] INFO: Crawled 24 pages (at 2 pages/min), scraped 6 items (at 3 items/min)
2018-10-24 10:43:50 [scrapy.extensions.logstats] INFO: Crawled 27 pages (at 3 pages/min), scraped 6 items (at 0 items/min)
2018-10-24 10:44:51 [scrapy.extensions.logstats] INFO: Crawled 31 pages (at 4 pages/min), scraped 6 items (at 0 items/min)
2018-10-24 10:45:06 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-24 10:45:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/urllib3.exceptions.ProtocolError': 1,
 'downloader/response_bytes': 1429546,
 'downloader/response_count': 32,
 'downloader/response_status_count/200': 32,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 24, 2, 45, 6, 496802),
 'item_scraped_count': 6,
 'log_count/ERROR': 10,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 32,
 'scheduler/dequeued': 33,
 'scheduler/dequeued/memory': 33,
 'scheduler/enqueued': 33,
 'scheduler/enqueued/memory': 33,
 'spider_exceptions/IndexError': 9,
 'start_time': datetime.datetime(2018, 10, 24, 2, 37, 44, 213912)}
2018-10-24 10:45:06 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-24 11:09:27 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-24 11:09:27 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-24 11:09:27 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-24 11:09:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-24 11:09:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-24 11:09:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-24 11:09:29 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-24 11:09:29 [scrapy.core.engine] INFO: Spider opened
2018-10-24 11:09:29 [py.warnings] WARNING: c:\users\admin\pycharmprojects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-24 14:50:45 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-24 14:50:45 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-24 14:50:45 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-24 14:50:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-24 14:50:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-24 14:50:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-24 14:50:47 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-24 14:50:47 [scrapy.core.engine] INFO: Spider opened
2018-10-24 14:50:47 [py.warnings] WARNING: c:\users\admin\pycharmprojects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-24 14:51:10 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-24 14:51:10 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-24 14:51:10 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-24 14:51:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-24 14:51:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-24 14:51:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-24 14:51:11 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-24 14:51:11 [scrapy.core.engine] INFO: Spider opened
2018-10-24 14:51:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-24 14:51:11 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-24 14:54:21 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 14 pages/min), scraped 0 items (at 0 items/min)
2018-10-24 14:54:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309614298653698229451> (referer: https://weibo.com/?category=7)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 49, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 14:54:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298651018076874> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 49, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 14:54:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298634089887452> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 49, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 14:54:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298262772365509> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 49, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 14:57:04 [scrapy.extensions.logstats] INFO: Crawled 26 pages (at 12 pages/min), scraped 5 items (at 5 items/min)
2018-10-24 14:57:18 [scrapy.extensions.logstats] INFO: Crawled 27 pages (at 1 pages/min), scraped 6 items (at 1 items/min)
2018-10-24 14:57:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298580235007243> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 49, in parse_item
    )
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 14:57:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298579480081212> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 49, in parse_item
    )
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 14:57:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298615735637461> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 49, in parse_item
    )
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 14:57:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298605191139421> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 49, in parse_item
    )
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 14:57:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298594965409120> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 49, in parse_item
    )
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 14:58:15 [scrapy.extensions.logstats] INFO: Crawled 31 pages (at 4 pages/min), scraped 8 items (at 2 items/min)
2018-10-24 14:58:57 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-24 14:58:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 1060726,
 'downloader/response_count': 34,
 'downloader/response_status_count/200': 34,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 24, 6, 58, 57, 167177),
 'item_scraped_count': 8,
 'log_count/ERROR': 9,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 34,
 'scheduler/dequeued': 34,
 'scheduler/dequeued/memory': 34,
 'scheduler/enqueued': 34,
 'scheduler/enqueued/memory': 34,
 'spider_exceptions/IndexError': 9,
 'start_time': datetime.datetime(2018, 10, 24, 6, 51, 11, 302110)}
2018-10-24 14:58:57 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-24 15:00:25 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-24 15:00:25 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-24 15:00:25 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-24 15:00:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-24 15:00:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-24 15:00:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-24 15:00:27 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-24 15:00:28 [scrapy.core.engine] INFO: Spider opened
2018-10-24 15:01:33 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-24 15:01:33 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-24 15:01:33 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-24 15:01:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-24 15:01:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-24 15:01:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-24 15:01:34 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-24 15:02:11 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-24 15:02:11 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-24 15:02:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-24 15:02:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-24 15:02:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-24 15:02:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-24 15:02:12 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-24 15:03:33 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-24 15:03:33 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-24 15:03:33 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-24 15:03:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-24 15:03:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-24 15:03:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-24 15:03:33 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-24 15:03:33 [scrapy.core.engine] INFO: Spider opened
2018-10-24 15:03:33 [py.warnings] WARNING: c:\users\admin\pycharmprojects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-29 18:12:08 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-29 18:12:08 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-29 18:12:08 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-29 18:12:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-29 18:12:09 [twisted] CRITICAL: Unhandled error in Deferred:
2018-10-29 18:12:09 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Python 3.6\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Python 3.6\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Python 3.6\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Python 3.6\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Python 3.6\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 994, in _gcd_import
  File "<frozen importlib._bootstrap>", line 971, in _find_and_load
  File "<frozen importlib._bootstrap>", line 955, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 5, in <module>
    from selenium import webdriver
ModuleNotFoundError: No module named 'selenium'
2018-10-29 18:13:31 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-29 18:13:31 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-29 18:13:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-29 18:13:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-29 18:13:32 [twisted] CRITICAL: Unhandled error in Deferred:
2018-10-29 18:13:32 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Python 3.6\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Python 3.6\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Python 3.6\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Python 3.6\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Python 3.6\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 994, in _gcd_import
  File "<frozen importlib._bootstrap>", line 971, in _find_and_load
  File "<frozen importlib._bootstrap>", line 955, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "C:\Python 3.6\lib\site-packages\scrapy\downloadermiddlewares\retry.py", line 20, in <module>
    from twisted.web.client import ResponseFailed
  File "C:\Python 3.6\lib\site-packages\twisted\web\client.py", line 41, in <module>
    from twisted.internet.endpoints import HostnameEndpoint, wrapClientTLS
  File "C:\Python 3.6\lib\site-packages\twisted\internet\endpoints.py", line 41, in <module>
    from twisted.internet.stdio import StandardIO, PipeAddress
  File "C:\Python 3.6\lib\site-packages\twisted\internet\stdio.py", line 30, in <module>
    from twisted.internet import _win32stdio
  File "C:\Python 3.6\lib\site-packages\twisted\internet\_win32stdio.py", line 9, in <module>
    import win32api
ModuleNotFoundError: No module named 'win32api'
2018-10-29 18:17:57 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-29 18:17:57 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-29 18:17:57 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-29 18:17:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-29 18:17:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-29 18:17:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-29 18:17:58 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-29 18:17:58 [scrapy.core.engine] INFO: Spider opened
2018-10-29 18:17:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-29 18:17:58 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=12>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=7>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=10018>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=10007>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=1760>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=novelty>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=99991>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=3&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=4&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=5&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=6&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=7&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=8&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=9&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=10&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=11&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=12&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-29 18:17:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 17,
 'downloader/exception_type_count/selenium.common.exceptions.WebDriverException': 17,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 29, 10, 17, 58, 686775),
 'log_count/ERROR': 17,
 'log_count/INFO': 7,
 'log_count/WARNING': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2018, 10, 29, 10, 17, 58, 313754)}
2018-10-29 18:17:58 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-29 18:18:56 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-29 18:18:56 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-29 18:18:56 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-29 18:18:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-29 18:18:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-29 18:18:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-29 18:18:56 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-29 18:18:56 [scrapy.core.engine] INFO: Spider opened
2018-10-29 18:18:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-29 18:18:56 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=12>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=7>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=10018>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=10007>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=1760>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=novelty>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=99991>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=3&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=4&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=5&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=6&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=7&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=8&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=9&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=10&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=11&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=12&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-29 18:18:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 17,
 'downloader/exception_type_count/selenium.common.exceptions.WebDriverException': 17,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 29, 10, 18, 57, 311129),
 'log_count/ERROR': 17,
 'log_count/INFO': 7,
 'log_count/WARNING': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2018, 10, 29, 10, 18, 56, 918106)}
2018-10-29 18:18:57 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-29 18:23:51 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-29 18:23:51 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-29 18:23:51 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-29 18:23:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-29 18:23:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-29 18:23:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-29 18:23:51 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-29 18:23:51 [scrapy.core.engine] INFO: Spider opened
2018-10-29 18:23:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-29 18:23:51 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-29 18:25:03 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2018-10-29 18:27:41 [scrapy.extensions.logstats] INFO: Crawled 19 pages (at 13 pages/min), scraped 1 items (at 1 items/min)
2018-10-29 18:27:53 [scrapy.extensions.logstats] INFO: Crawled 20 pages (at 1 pages/min), scraped 3 items (at 2 items/min)
2018-10-29 18:28:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300497539439050> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-29 18:28:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300514819995530> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-29 18:28:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300433421137672> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-29 18:28:52 [scrapy.extensions.logstats] INFO: Crawled 25 pages (at 5 pages/min), scraped 7 items (at 4 items/min)
2018-10-29 18:29:17 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-29 18:29:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 1375860,
 'downloader/response_count': 27,
 'downloader/response_status_count/200': 27,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 29, 10, 29, 17, 340592),
 'item_scraped_count': 7,
 'log_count/ERROR': 3,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 27,
 'scheduler/dequeued': 27,
 'scheduler/dequeued/memory': 27,
 'scheduler/enqueued': 27,
 'scheduler/enqueued/memory': 27,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2018, 10, 29, 10, 23, 51, 840975)}
2018-10-29 18:29:17 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 10:42:22 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-30 10:42:22 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 10:42:22 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-30 10:42:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 10:42:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 10:42:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 10:42:22 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-30 10:42:22 [scrapy.core.engine] INFO: Spider opened
2018-10-30 10:42:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 10:42:22 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-30 10:43:29 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 10:45:35 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 10:46:29 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 10:47:26 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 10:48:31 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 10:49:34 [scrapy.extensions.logstats] INFO: Crawled 17 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 10:49:34 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 10:49:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 347175,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 2, 49, 34, 227614),
 'log_count/INFO': 13,
 'log_count/WARNING': 1,
 'response_received_count': 17,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2018, 10, 30, 2, 42, 22, 534922)}
2018-10-30 10:49:34 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 10:50:12 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-30 10:50:12 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 10:50:12 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-30 10:50:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 10:50:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 10:50:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 10:50:13 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-30 10:50:13 [scrapy.core.engine] INFO: Spider opened
2018-10-30 10:50:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 10:50:13 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-30 10:51:20 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 10:52:24 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 10:53:19 [scrapy.extensions.logstats] INFO: Crawled 12 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 10:54:29 [scrapy.extensions.logstats] INFO: Crawled 16 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 10:54:42 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 10:54:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 348279,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 2, 54, 42, 664255),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 17,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2018, 10, 30, 2, 50, 13, 352852)}
2018-10-30 10:54:42 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 11:11:41 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-30 11:11:41 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 11:11:41 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-30 11:11:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 11:11:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 11:11:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 11:11:41 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-30 11:11:41 [scrapy.core.engine] INFO: Spider opened
2018-10-30 11:11:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 11:11:41 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-30 11:14:17 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 14 pages/min), scraped 0 items (at 0 items/min)
