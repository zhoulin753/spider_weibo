2018-10-22 08:57:31 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 08:57:31 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 08:57:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 08:57:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 08:57:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 08:57:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 08:57:31 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 08:57:31 [scrapy.core.engine] INFO: Spider opened
2018-10-22 08:57:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 08:57:31 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 09:00:39 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 13 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:01:39 [scrapy.extensions.logstats] INFO: Crawled 16 pages (at 3 pages/min), scraped 8 items (at 8 items/min)
2018-10-22 09:02:44 [scrapy.extensions.logstats] INFO: Crawled 20 pages (at 4 pages/min), scraped 8 items (at 0 items/min)
2018-10-22 09:03:18 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 09:03:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 771544,
 'downloader/response_count': 22,
 'downloader/response_status_count/200': 22,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 1, 3, 18, 359565),
 'item_scraped_count': 8,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 22,
 'scheduler/dequeued': 22,
 'scheduler/dequeued/memory': 22,
 'scheduler/enqueued': 22,
 'scheduler/enqueued/memory': 22,
 'start_time': datetime.datetime(2018, 10, 22, 0, 57, 31, 623560)}
2018-10-22 09:03:18 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 09:07:34 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 09:07:34 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 09:07:34 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 09:07:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 09:07:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 09:07:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 09:07:35 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 09:07:35 [scrapy.core.engine] INFO: Spider opened
2018-10-22 09:07:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:07:35 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 09:08:40 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:09:36 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:10:42 [scrapy.extensions.logstats] INFO: Crawled 12 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:11:14 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 09:11:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 700396,
 'downloader/response_count': 14,
 'downloader/response_status_count/200': 14,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 1, 11, 14, 417503),
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 14,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2018, 10, 22, 1, 7, 35, 66934)}
2018-10-22 09:11:14 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 09:13:03 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 09:13:03 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 09:13:03 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 09:13:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 09:13:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 09:13:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 09:13:04 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 09:13:04 [scrapy.core.engine] INFO: Spider opened
2018-10-22 09:13:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:13:04 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 09:14:30 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:16:35 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 09:16:35 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 09:16:35 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 09:16:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 09:16:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 09:16:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 09:16:35 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 09:16:35 [scrapy.core.engine] INFO: Spider opened
2018-10-22 09:16:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:16:35 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 09:17:44 [scrapy.extensions.logstats] INFO: Crawled 7 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:18:45 [scrapy.extensions.logstats] INFO: Crawled 12 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:19:10 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 09:19:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 346171,
 'downloader/response_count': 14,
 'downloader/response_status_count/200': 14,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 1, 19, 10, 774908),
 'log_count/INFO': 9,
 'log_count/WARNING': 1,
 'response_received_count': 14,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2018, 10, 22, 1, 16, 35, 551709)}
2018-10-22 09:19:10 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 09:20:01 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 09:20:01 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 09:20:01 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 09:20:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 09:20:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 09:20:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 09:20:01 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 09:20:01 [scrapy.core.engine] INFO: Spider opened
2018-10-22 09:20:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:20:01 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 09:21:03 [scrapy.extensions.logstats] INFO: Crawled 7 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:22:05 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:22:05 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 09:22:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 342141,
 'downloader/response_count': 14,
 'downloader/response_status_count/200': 14,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 1, 22, 5, 691227),
 'log_count/INFO': 9,
 'log_count/WARNING': 1,
 'response_received_count': 14,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2018, 10, 22, 1, 20, 1, 845091)}
2018-10-22 09:22:05 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 09:31:16 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 09:31:16 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 09:31:16 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 09:31:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 09:31:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 09:31:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 09:31:17 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 09:31:17 [scrapy.core.engine] INFO: Spider opened
2018-10-22 09:31:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:31:17 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 09:32:25 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 8 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:33:17 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:33:17 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 09:33:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 346172,
 'downloader/response_count': 14,
 'downloader/response_status_count/200': 14,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 1, 33, 17, 484669),
 'log_count/INFO': 9,
 'log_count/WARNING': 1,
 'response_received_count': 14,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2018, 10, 22, 1, 31, 17, 117794)}
2018-10-22 09:33:17 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 09:46:42 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 09:46:42 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 09:46:42 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 09:46:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-22 09:46:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 09:46:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 09:46:42 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 09:51:31 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 09:51:31 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 09:51:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 09:51:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-22 09:51:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 09:51:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 09:51:31 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 09:51:31 [scrapy.core.engine] INFO: Spider opened
2018-10-22 09:51:32 [py.warnings] WARNING: c:\users\admin\pycharmprojects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 09:54:27 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 09:54:27 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 09:54:27 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 09:54:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 09:54:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 09:54:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 09:54:27 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 09:54:27 [scrapy.core.engine] INFO: Spider opened
2018-10-22 09:54:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:54:27 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 09:55:28 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:56:32 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 09:56:48 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 09:56:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 346003,
 'downloader/response_count': 14,
 'downloader/response_status_count/200': 14,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 1, 56, 48, 213282),
 'log_count/INFO': 9,
 'log_count/WARNING': 1,
 'response_received_count': 14,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2018, 10, 22, 1, 54, 27, 546806)}
2018-10-22 09:56:48 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 10:35:51 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 10:35:51 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 10:35:51 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 10:35:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 10:35:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 10:35:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 10:35:51 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 10:35:51 [scrapy.core.engine] INFO: Spider opened
2018-10-22 10:35:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 10:35:51 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 10:37:01 [scrapy.extensions.logstats] INFO: Crawled 7 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 10:38:00 [scrapy.extensions.logstats] INFO: Crawled 12 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 10:38:24 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 10:38:24 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 346354,
 'downloader/response_count': 14,
 'downloader/response_status_count/200': 14,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 2, 38, 24, 39269),
 'log_count/INFO': 9,
 'log_count/WARNING': 1,
 'response_received_count': 14,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2018, 10, 22, 2, 35, 51, 942058)}
2018-10-22 10:38:24 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 10:38:31 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 10:38:31 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 10:38:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 10:38:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 10:38:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 10:38:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 10:38:32 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 10:38:32 [scrapy.core.engine] INFO: Spider opened
2018-10-22 10:38:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 10:38:32 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 10:39:34 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 10:40:33 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 10:40:41 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 10:40:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 346354,
 'downloader/response_count': 14,
 'downloader/response_status_count/200': 14,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 2, 40, 41, 892971),
 'log_count/INFO': 9,
 'log_count/WARNING': 1,
 'response_received_count': 14,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2018, 10, 22, 2, 38, 32, 298371)}
2018-10-22 10:40:41 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 10:41:25 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 10:41:25 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 10:41:25 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 10:41:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 10:41:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 10:41:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 10:41:26 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 10:41:26 [scrapy.core.engine] INFO: Spider opened
2018-10-22 10:41:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 10:41:26 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 10:42:30 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 10:43:36 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 10:43:44 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 10:43:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 346354,
 'downloader/response_count': 14,
 'downloader/response_status_count/200': 14,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 2, 43, 44, 533552),
 'log_count/INFO': 9,
 'log_count/WARNING': 1,
 'response_received_count': 14,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2018, 10, 22, 2, 41, 26, 225994)}
2018-10-22 10:43:44 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 14:47:24 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 14:47:24 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 14:47:24 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 14:47:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 14:47:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 14:47:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 14:47:26 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 14:47:26 [scrapy.core.engine] INFO: Spider opened
2018-10-22 14:47:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 14:47:27 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 14:48:28 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 14:48:53 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 14:48:53 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 14:48:53 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 14:48:53 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 14:48:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 14:48:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 14:48:54 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 14:48:54 [scrapy.core.engine] INFO: Spider opened
2018-10-22 14:48:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 14:48:54 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 14:49:25 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 14:49:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 26322,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 6, 49, 25, 392434),
 'log_count/INFO': 7,
 'log_count/WARNING': 1,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2018, 10, 22, 6, 48, 54, 217364)}
2018-10-22 14:49:25 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 14:49:47 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 14:49:47 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 14:49:47 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 14:49:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 14:49:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 14:49:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 14:49:48 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 14:49:48 [scrapy.core.engine] INFO: Spider opened
2018-10-22 14:49:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 14:49:48 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 14:50:34 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 14:50:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 386397,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 6, 50, 34, 81059),
 'log_count/INFO': 7,
 'log_count/WARNING': 1,
 'response_received_count': 4,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2018, 10, 22, 6, 49, 48, 222578)}
2018-10-22 14:50:34 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 14:52:08 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 14:52:08 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 14:52:08 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 14:52:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 14:52:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 14:52:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 14:52:09 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 14:52:09 [scrapy.core.engine] INFO: Spider opened
2018-10-22 14:52:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 14:52:09 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 14:53:16 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 14:53:16 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 14:53:16 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 14:53:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-22 14:53:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 14:53:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 14:53:16 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 14:53:16 [scrapy.core.engine] INFO: Spider opened
2018-10-22 14:53:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 14:53:16 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-22 14:54:31 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 14:56:20 [scrapy.extensions.logstats] INFO: Crawled 15 pages (at 9 pages/min), scraped 0 items (at 0 items/min)
2018-10-22 14:57:21 [scrapy.extensions.logstats] INFO: Crawled 20 pages (at 5 pages/min), scraped 8 items (at 8 items/min)
2018-10-22 14:57:46 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-22 14:57:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 706599,
 'downloader/response_count': 22,
 'downloader/response_status_count/200': 22,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 22, 6, 57, 46, 775237),
 'item_scraped_count': 8,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 22,
 'scheduler/dequeued': 22,
 'scheduler/dequeued/memory': 22,
 'scheduler/enqueued': 22,
 'scheduler/enqueued/memory': 22,
 'start_time': datetime.datetime(2018, 10, 22, 6, 53, 16, 535263)}
2018-10-22 14:57:46 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-22 15:04:16 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-22 15:04:16 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-22 15:04:16 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-22 15:04:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-22 15:04:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-22 15:04:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-22 15:04:17 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-22 15:04:17 [scrapy.core.engine] INFO: Spider opened
2018-10-22 15:04:17 [py.warnings] WARNING: c:\users\admin\pycharmprojects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-23 11:30:17 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-23 11:30:17 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-23 11:30:17 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-23 11:30:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-23 11:30:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-23 11:30:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-23 11:30:19 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-23 11:30:47 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-23 11:30:47 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-23 11:30:47 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-23 11:30:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-23 11:30:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-23 11:30:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-23 11:30:47 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-23 11:30:47 [scrapy.core.engine] INFO: Spider opened
2018-10-23 11:30:47 [py.warnings] WARNING: c:\users\admin\pycharmprojects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-23 11:40:37 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-23 11:40:37 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-23 11:40:37 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-23 11:40:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-23 11:40:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-23 11:40:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-23 11:40:38 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-23 11:41:32 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-23 11:41:32 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-23 11:41:32 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-23 11:41:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-23 11:41:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-23 11:41:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-23 11:41:33 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-23 11:42:43 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-23 11:42:43 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-23 11:42:43 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-23 11:42:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-23 11:42:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-23 11:42:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-23 11:42:44 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-23 11:42:44 [scrapy.core.engine] INFO: Spider opened
2018-10-23 11:42:44 [py.warnings] WARNING: c:\users\admin\pycharmprojects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-23 19:12:18 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-23 19:12:18 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-23 19:12:18 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-23 19:12:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-23 19:12:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-23 19:12:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-23 19:12:20 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-23 19:12:20 [scrapy.core.engine] INFO: Spider opened
2018-10-23 19:12:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-23 19:12:20 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-23 19:13:44 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2018-10-23 19:16:02 [scrapy.extensions.logstats] INFO: Crawled 15 pages (at 9 pages/min), scraped 0 items (at 0 items/min)
2018-10-23 19:16:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298337355482343> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 41, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 50, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-23 19:16:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298268774388807> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 41, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 50, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-23 19:16:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298240257305277> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 41, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 50, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-23 19:16:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298328660647409> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 41, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 50, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-23 19:16:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298303071242458> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 41, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 50, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-23 19:16:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298245621821033> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 41, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 50, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-23 19:16:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298286046532123> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 41, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 50, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-23 19:16:21 [scrapy.extensions.logstats] INFO: Crawled 16 pages (at 1 pages/min), scraped 1 items (at 1 items/min)
2018-10-23 19:17:26 [scrapy.extensions.logstats] INFO: Crawled 21 pages (at 5 pages/min), scraped 1 items (at 0 items/min)
2018-10-23 19:17:40 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-23 19:17:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 549468,
 'downloader/response_count': 22,
 'downloader/response_status_count/200': 22,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 23, 11, 17, 40, 639068),
 'item_scraped_count': 1,
 'log_count/ERROR': 7,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 22,
 'scheduler/dequeued': 22,
 'scheduler/dequeued/memory': 22,
 'scheduler/enqueued': 22,
 'scheduler/enqueued/memory': 22,
 'spider_exceptions/IndexError': 7,
 'start_time': datetime.datetime(2018, 10, 23, 11, 12, 20, 568998)}
2018-10-23 19:17:40 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-24 10:01:07 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-24 10:01:07 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-24 10:01:07 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-24 10:01:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-24 10:01:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-24 10:01:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-24 10:01:09 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-24 10:01:09 [scrapy.core.engine] INFO: Spider opened
2018-10-24 10:01:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-24 10:01:09 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-24 10:04:06 [scrapy.extensions.logstats] INFO: Crawled 15 pages (at 15 pages/min), scraped 0 items (at 0 items/min)
2018-10-24 10:07:35 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-24 10:07:35 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-24 10:07:35 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-24 10:07:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-24 10:07:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-24 10:07:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-24 10:07:36 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-24 10:07:36 [scrapy.core.engine] INFO: Spider opened
2018-10-24 10:07:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-24 10:07:36 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-24 10:10:33 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 13 pages/min), scraped 0 items (at 0 items/min)
2018-10-24 10:10:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298576351129119> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 41, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 10:10:49 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2018-10-24 10:11:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298579383609650> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 41, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 10:11:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298576472766885> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 41, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 10:11:50 [scrapy.extensions.logstats] INFO: Crawled 18 pages (at 4 pages/min), scraped 5 items (at 5 items/min)
2018-10-24 10:13:03 [scrapy.extensions.logstats] INFO: Crawled 21 pages (at 3 pages/min), scraped 5 items (at 0 items/min)
2018-10-24 10:13:18 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-24 10:13:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 1049750,
 'downloader/response_count': 22,
 'downloader/response_status_count/200': 22,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 24, 2, 13, 18, 61350),
 'item_scraped_count': 5,
 'log_count/ERROR': 3,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 22,
 'scheduler/dequeued': 22,
 'scheduler/dequeued/memory': 22,
 'scheduler/enqueued': 22,
 'scheduler/enqueued/memory': 22,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2018, 10, 24, 2, 7, 36, 770469)}
2018-10-24 10:13:18 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-24 10:37:42 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-24 10:37:42 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-24 10:37:42 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-24 10:37:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-24 10:37:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-24 10:37:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-24 10:37:44 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-24 10:37:44 [scrapy.core.engine] INFO: Spider opened
2018-10-24 10:37:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-24 10:37:44 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-24 10:40:08 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 13 pages/min), scraped 0 items (at 0 items/min)
2018-10-24 10:40:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298589714154908> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 46, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 10:42:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298584286727740> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 46, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 10:42:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298587466038239> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 46, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 10:42:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298589395396497> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 46, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 10:42:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298587268898129> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 46, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 10:42:06 [scrapy.extensions.logstats] INFO: Crawled 22 pages (at 9 pages/min), scraped 3 items (at 3 items/min)
2018-10-24 10:42:23 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/ttarticle/p/show?id=2309404298581426215461>
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\urllib3\connectionpool.py", line 384, in _make_request
    six.raise_from(e, None)
  File "<string>", line 2, in raise_from
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\urllib3\connectionpool.py", line 380, in _make_request
    httplib_response = conn.getresponse()
  File "C:\Python36\lib\http\client.py", line 1331, in getresponse
    response.begin()
  File "C:\Python36\lib\http\client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "C:\Python36\lib\http\client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "C:\Python36\lib\socket.py", line 586, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\middlewares\middleware.py", line 19, in process_request
    driver.get(request.url)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 319, in execute
    response = self.command_executor.execute(driver_command, params)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\remote\remote_connection.py", line 376, in execute
    return self._request(command_info[0], url, body=data)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\remote\remote_connection.py", line 404, in _request
    resp = http.request(method, url, body=body, headers=headers)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\urllib3\request.py", line 72, in request
    **urlopen_kw)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\urllib3\request.py", line 150, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\urllib3\poolmanager.py", line 322, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\urllib3\util\retry.py", line 367, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\urllib3\packages\six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\urllib3\connectionpool.py", line 384, in _make_request
    six.raise_from(e, None)
  File "<string>", line 2, in raise_from
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\urllib3\connectionpool.py", line 380, in _make_request
    httplib_response = conn.getresponse()
  File "C:\Python36\lib\http\client.py", line 1331, in getresponse
    response.begin()
  File "C:\Python36\lib\http\client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "C:\Python36\lib\http\client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "C:\Python36\lib\socket.py", line 586, in readinto
    return self._sock.recv_into(b)
urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None))
2018-10-24 10:42:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298589458312259> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 46, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 10:43:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298582751581461> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 46, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 10:43:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298575797469703> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 46, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 10:43:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298576997002546> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 46, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 10:43:06 [scrapy.extensions.logstats] INFO: Crawled 24 pages (at 2 pages/min), scraped 6 items (at 3 items/min)
2018-10-24 10:43:50 [scrapy.extensions.logstats] INFO: Crawled 27 pages (at 3 pages/min), scraped 6 items (at 0 items/min)
2018-10-24 10:44:51 [scrapy.extensions.logstats] INFO: Crawled 31 pages (at 4 pages/min), scraped 6 items (at 0 items/min)
2018-10-24 10:45:06 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-24 10:45:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/urllib3.exceptions.ProtocolError': 1,
 'downloader/response_bytes': 1429546,
 'downloader/response_count': 32,
 'downloader/response_status_count/200': 32,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 24, 2, 45, 6, 496802),
 'item_scraped_count': 6,
 'log_count/ERROR': 10,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 32,
 'scheduler/dequeued': 33,
 'scheduler/dequeued/memory': 33,
 'scheduler/enqueued': 33,
 'scheduler/enqueued/memory': 33,
 'spider_exceptions/IndexError': 9,
 'start_time': datetime.datetime(2018, 10, 24, 2, 37, 44, 213912)}
2018-10-24 10:45:06 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-24 11:09:27 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-24 11:09:27 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-24 11:09:27 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-24 11:09:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-24 11:09:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-24 11:09:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-24 11:09:29 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-24 11:09:29 [scrapy.core.engine] INFO: Spider opened
2018-10-24 11:09:29 [py.warnings] WARNING: c:\users\admin\pycharmprojects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-24 14:50:45 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-24 14:50:45 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-24 14:50:45 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-24 14:50:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-24 14:50:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-24 14:50:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-24 14:50:47 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-24 14:50:47 [scrapy.core.engine] INFO: Spider opened
2018-10-24 14:50:47 [py.warnings] WARNING: c:\users\admin\pycharmprojects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-24 14:51:10 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-24 14:51:10 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-24 14:51:10 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-24 14:51:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-24 14:51:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-24 14:51:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-24 14:51:11 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-24 14:51:11 [scrapy.core.engine] INFO: Spider opened
2018-10-24 14:51:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-24 14:51:11 [py.warnings] WARNING: C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-24 14:54:21 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 14 pages/min), scraped 0 items (at 0 items/min)
2018-10-24 14:54:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309614298653698229451> (referer: https://weibo.com/?category=7)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 49, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 14:54:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298651018076874> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 49, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 14:54:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298634089887452> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 49, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 14:54:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298262772365509> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 49, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 14:57:04 [scrapy.extensions.logstats] INFO: Crawled 26 pages (at 12 pages/min), scraped 5 items (at 5 items/min)
2018-10-24 14:57:18 [scrapy.extensions.logstats] INFO: Crawled 27 pages (at 1 pages/min), scraped 6 items (at 1 items/min)
2018-10-24 14:57:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298580235007243> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 49, in parse_item
    )
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 14:57:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298579480081212> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 49, in parse_item
    )
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 14:57:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298615735637461> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 49, in parse_item
    )
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 14:57:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298605191139421> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 49, in parse_item
    )
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 14:57:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404298594965409120> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\scrapy\spiders\crawl.py", line 82, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\admin\PycharmProjects\spider-dome\weibo\weibo\spiders\text.py", line 49, in parse_item
    )
  File "C:\Users\admin\PycharmProjects\text\venv\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-24 14:58:15 [scrapy.extensions.logstats] INFO: Crawled 31 pages (at 4 pages/min), scraped 8 items (at 2 items/min)
2018-10-24 14:58:57 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-24 14:58:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 1060726,
 'downloader/response_count': 34,
 'downloader/response_status_count/200': 34,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 24, 6, 58, 57, 167177),
 'item_scraped_count': 8,
 'log_count/ERROR': 9,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 34,
 'scheduler/dequeued': 34,
 'scheduler/dequeued/memory': 34,
 'scheduler/enqueued': 34,
 'scheduler/enqueued/memory': 34,
 'spider_exceptions/IndexError': 9,
 'start_time': datetime.datetime(2018, 10, 24, 6, 51, 11, 302110)}
2018-10-24 14:58:57 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-24 15:00:25 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-24 15:00:25 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-24 15:00:25 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-24 15:00:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-24 15:00:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-24 15:00:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-24 15:00:27 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-24 15:00:28 [scrapy.core.engine] INFO: Spider opened
2018-10-24 15:01:33 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-24 15:01:33 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-24 15:01:33 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-24 15:01:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-24 15:01:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-24 15:01:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-24 15:01:34 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-24 15:02:11 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-24 15:02:11 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-24 15:02:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-24 15:02:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-24 15:02:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-24 15:02:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-24 15:02:12 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-24 15:03:33 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-24 15:03:33 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Windows-10-10.0.17134-SP0
2018-10-24 15:03:33 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-24 15:03:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2018-10-24 15:03:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-24 15:03:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-24 15:03:33 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-24 15:03:33 [scrapy.core.engine] INFO: Spider opened
2018-10-24 15:03:33 [py.warnings] WARNING: c:\users\admin\pycharmprojects\text\venv\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-29 18:12:08 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-29 18:12:08 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-29 18:12:08 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-29 18:12:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-29 18:12:09 [twisted] CRITICAL: Unhandled error in Deferred:
2018-10-29 18:12:09 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Python 3.6\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Python 3.6\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Python 3.6\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Python 3.6\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Python 3.6\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 994, in _gcd_import
  File "<frozen importlib._bootstrap>", line 971, in _find_and_load
  File "<frozen importlib._bootstrap>", line 955, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 5, in <module>
    from selenium import webdriver
ModuleNotFoundError: No module named 'selenium'
2018-10-29 18:13:31 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-29 18:13:31 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-29 18:13:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-29 18:13:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-29 18:13:32 [twisted] CRITICAL: Unhandled error in Deferred:
2018-10-29 18:13:32 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\crawler.py", line 80, in crawl
    self.engine = self._create_engine()
  File "C:\Python 3.6\lib\site-packages\scrapy\crawler.py", line 105, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "C:\Python 3.6\lib\site-packages\scrapy\core\engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "C:\Python 3.6\lib\site-packages\scrapy\middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "C:\Python 3.6\lib\site-packages\scrapy\middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\misc.py", line 44, in load_object
    mod = import_module(module)
  File "C:\Python 3.6\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 994, in _gcd_import
  File "<frozen importlib._bootstrap>", line 971, in _find_and_load
  File "<frozen importlib._bootstrap>", line 955, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "C:\Python 3.6\lib\site-packages\scrapy\downloadermiddlewares\retry.py", line 20, in <module>
    from twisted.web.client import ResponseFailed
  File "C:\Python 3.6\lib\site-packages\twisted\web\client.py", line 41, in <module>
    from twisted.internet.endpoints import HostnameEndpoint, wrapClientTLS
  File "C:\Python 3.6\lib\site-packages\twisted\internet\endpoints.py", line 41, in <module>
    from twisted.internet.stdio import StandardIO, PipeAddress
  File "C:\Python 3.6\lib\site-packages\twisted\internet\stdio.py", line 30, in <module>
    from twisted.internet import _win32stdio
  File "C:\Python 3.6\lib\site-packages\twisted\internet\_win32stdio.py", line 9, in <module>
    import win32api
ModuleNotFoundError: No module named 'win32api'
2018-10-29 18:17:57 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-29 18:17:57 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-29 18:17:57 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-29 18:17:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-29 18:17:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-29 18:17:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-29 18:17:58 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-29 18:17:58 [scrapy.core.engine] INFO: Spider opened
2018-10-29 18:17:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-29 18:17:58 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=12>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=7>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=10018>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=10007>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=1760>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=novelty>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=99991>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=3&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=4&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=5&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=6&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=7&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=8&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=9&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=10&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=11&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=12&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:17:58 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-29 18:17:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 17,
 'downloader/exception_type_count/selenium.common.exceptions.WebDriverException': 17,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 29, 10, 17, 58, 686775),
 'log_count/ERROR': 17,
 'log_count/INFO': 7,
 'log_count/WARNING': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2018, 10, 29, 10, 17, 58, 313754)}
2018-10-29 18:17:58 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-29 18:18:56 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-29 18:18:56 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-29 18:18:56 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-29 18:18:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-29 18:18:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-29 18:18:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-29 18:18:56 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-29 18:18:56 [scrapy.core.engine] INFO: Spider opened
2018-10-29 18:18:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-29 18:18:56 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=12>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=7>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=10018>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=10007>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=1760>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=novelty>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/?category=99991>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=3&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=4&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=5&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=6&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=7&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=8&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=9&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=10&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=11&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.scraper] ERROR: Error downloading <GET https://weibo.com/a/aj/transform/loadingmoreunlogin?ajwvr=6&category=1760&page=12&lefnav=0&cursor=>
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 76, in start
    stdin=PIPE)
  File "C:\Python 3.6\lib\subprocess.py", line 709, in __init__
    restore_signals, start_new_session)
  File "C:\Python 3.6\lib\subprocess.py", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Python 3.6\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\middlewares\middleware.py", line 17, in process_request
    r"C:\Users\admin\Downloads\phantomjs-2.1.1-windows\phantomjs-2.1.1-windows\bin\phantomjs")  # 指定使用的浏览器
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py", line 56, in __init__
    self.service.start()
  File "C:\Python 3.6\lib\site-packages\selenium\webdriver\common\service.py", line 83, in start
    os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'phantomjs' executable needs to be in PATH. 

2018-10-29 18:18:57 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-29 18:18:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 17,
 'downloader/exception_type_count/selenium.common.exceptions.WebDriverException': 17,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 29, 10, 18, 57, 311129),
 'log_count/ERROR': 17,
 'log_count/INFO': 7,
 'log_count/WARNING': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2018, 10, 29, 10, 18, 56, 918106)}
2018-10-29 18:18:57 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-29 18:23:51 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-29 18:23:51 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-29 18:23:51 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-29 18:23:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-29 18:23:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-29 18:23:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-29 18:23:51 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-29 18:23:51 [scrapy.core.engine] INFO: Spider opened
2018-10-29 18:23:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-29 18:23:51 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-29 18:25:03 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2018-10-29 18:27:41 [scrapy.extensions.logstats] INFO: Crawled 19 pages (at 13 pages/min), scraped 1 items (at 1 items/min)
2018-10-29 18:27:53 [scrapy.extensions.logstats] INFO: Crawled 20 pages (at 1 pages/min), scraped 3 items (at 2 items/min)
2018-10-29 18:28:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300497539439050> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-29 18:28:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300514819995530> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-29 18:28:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300433421137672> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-29 18:28:52 [scrapy.extensions.logstats] INFO: Crawled 25 pages (at 5 pages/min), scraped 7 items (at 4 items/min)
2018-10-29 18:29:17 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-29 18:29:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 1375860,
 'downloader/response_count': 27,
 'downloader/response_status_count/200': 27,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 29, 10, 29, 17, 340592),
 'item_scraped_count': 7,
 'log_count/ERROR': 3,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 27,
 'scheduler/dequeued': 27,
 'scheduler/dequeued/memory': 27,
 'scheduler/enqueued': 27,
 'scheduler/enqueued/memory': 27,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2018, 10, 29, 10, 23, 51, 840975)}
2018-10-29 18:29:17 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 10:42:22 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-30 10:42:22 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 10:42:22 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-30 10:42:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 10:42:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 10:42:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 10:42:22 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-30 10:42:22 [scrapy.core.engine] INFO: Spider opened
2018-10-30 10:42:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 10:42:22 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-30 10:43:29 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 10:45:35 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 10:46:29 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 10:47:26 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 10:48:31 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 10:49:34 [scrapy.extensions.logstats] INFO: Crawled 17 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 10:49:34 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 10:49:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 347175,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 2, 49, 34, 227614),
 'log_count/INFO': 13,
 'log_count/WARNING': 1,
 'response_received_count': 17,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2018, 10, 30, 2, 42, 22, 534922)}
2018-10-30 10:49:34 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 10:50:12 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-30 10:50:12 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 10:50:12 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-30 10:50:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 10:50:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 10:50:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 10:50:13 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-30 10:50:13 [scrapy.core.engine] INFO: Spider opened
2018-10-30 10:50:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 10:50:13 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-30 10:51:20 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 10:52:24 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 10:53:19 [scrapy.extensions.logstats] INFO: Crawled 12 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 10:54:29 [scrapy.extensions.logstats] INFO: Crawled 16 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 10:54:42 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 10:54:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 348279,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 2, 54, 42, 664255),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 17,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2018, 10, 30, 2, 50, 13, 352852)}
2018-10-30 10:54:42 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 11:11:41 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-30 11:11:41 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 11:11:41 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-30 11:11:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 11:11:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 11:11:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 11:11:41 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-30 11:11:41 [scrapy.core.engine] INFO: Spider opened
2018-10-30 11:11:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 11:11:41 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-30 11:14:17 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 14 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 11:16:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309614300760685907040> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 58, in parse_item
    author = response.xpath(r"//em[@class='W_autocut']/text()")[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:16:18 [scrapy.extensions.logstats] INFO: Crawled 25 pages (at 11 pages/min), scraped 8 items (at 8 items/min)
2018-10-30 11:18:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300771796587060> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:18:20 [scrapy.extensions.logstats] INFO: Crawled 36 pages (at 11 pages/min), scraped 16 items (at 8 items/min)
2018-10-30 11:23:11 [scrapy.extensions.logstats] INFO: Crawled 62 pages (at 26 pages/min), scraped 17 items (at 1 items/min)
2018-10-30 11:23:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300739030715320> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:23:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7557938375860225_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:23:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7546555141724161_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:23:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7548451532838913_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:23:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7548764968982529_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:23:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7553269467486209_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:23:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7559971644446721_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:23:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7552439149434881_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:23:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7552938756577281_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:23:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7552947035609089_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:23:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7555978301052929_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:24:07 [scrapy.extensions.logstats] INFO: Crawled 67 pages (at 5 pages/min), scraped 23 items (at 6 items/min)
2018-10-30 11:24:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7549059426424833_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:24:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7554617644193793_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:24:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7548451904034817_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:24:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7547136937826305_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:24:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7555014120740865_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:24:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7548185109600257_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:24:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7554560757897217_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:24:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7555752466618369_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:24:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7561017597401089_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:24:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7560568912289793_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:24:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7552440440756225_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:24:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7555892638160897_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:24:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7546503505647617_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:24:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7559953779302401_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:24:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7561677985323009_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:27:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7553026057345025_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:27:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7554204520978433_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:27:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7553182390065153_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:27:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7548817085831169_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:28:11 [scrapy.extensions.logstats] INFO: Crawled 88 pages (at 21 pages/min), scraped 23 items (at 0 items/min)
2018-10-30 11:28:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7556962416367617_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:28:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7549043090135041_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:28:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7548786506733569_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:28:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7548718649186305_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:28:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7548728988669953_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:28:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7555164339740673_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:28:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7548290824935425_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:28:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7555292204144641_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:28:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7554585844029441_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:28:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7555970961022977_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:28:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7561881102882817_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:28:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7557711956842497_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:28:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7549268256626689_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:28:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7558001848262657_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:28:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7548696009906177_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:28:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7548874864990209_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:28:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7552680420481025_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:28:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7558917223651329_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:28:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7549083971491841_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:28:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7556441735469057_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:28:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7549082161125377_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in parse_item
    r'//div[@node-type="articleTitle"]/text()')[0].extract()
  File "C:\Python 3.6\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2018-10-30 11:28:49 [scrapy.extensions.logstats] INFO: Crawled 91 pages (at 3 pages/min), scraped 23 items (at 0 items/min)
2018-10-30 11:29:11 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 11:29:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 9579531,
 'downloader/response_count': 93,
 'downloader/response_status_count/200': 93,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 3, 29, 11, 843606),
 'item_scraped_count': 23,
 'log_count/ERROR': 53,
 'log_count/INFO': 14,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 93,
 'scheduler/dequeued': 93,
 'scheduler/dequeued/memory': 93,
 'scheduler/enqueued': 93,
 'scheduler/enqueued/memory': 93,
 'spider_exceptions/IndexError': 53,
 'start_time': datetime.datetime(2018, 10, 30, 3, 11, 41, 875551)}
2018-10-30 11:29:11 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 11:55:43 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-30 11:55:43 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 11:55:43 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-30 11:55:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 11:55:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 11:55:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 11:55:44 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-30 11:55:44 [scrapy.core.engine] INFO: Spider opened
2018-10-30 11:55:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 11:55:44 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-30 11:56:42 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-30 11:56:42 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 11:56:42 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-30 11:56:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 11:56:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 11:56:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 11:56:43 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-30 11:56:43 [scrapy.core.engine] INFO: Spider opened
2018-10-30 11:56:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 11:56:43 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-30 11:57:48 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 12:00:22 [scrapy.extensions.logstats] INFO: Crawled 17 pages (at 12 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 12:00:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300768718027841> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:02:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300761805750530> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:02:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300773721820637> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:02:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300776351623545> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:02:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300767241600110> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:02:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300769682683551> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:02:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300774359315173> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:02:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300766352433383> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:05:48 [scrapy.extensions.logstats] INFO: Crawled 42 pages (at 25 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 12:05:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300712153641624> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:09:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7563169927567361_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:09:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300739030715320> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:09:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300718231133245> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:11:33 [scrapy.extensions.logstats] INFO: Crawled 68 pages (at 26 pages/min), scraped 5 items (at 5 items/min)
2018-10-30 12:11:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7549501663352833_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:11:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7549279238363137_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:11:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7554120768591873_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:11:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7548826766284801_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:11:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7560703777478657_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:11:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7562774552025089_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:11:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7563213125261313_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:11:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7563188883200001_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:11:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7563166652864513_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:11:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7561672593053697_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:11:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7544867331250177_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:11:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7563210485471233_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:11:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7562265474177025_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:11:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7563125369935873_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:11:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7563257397680129_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:13:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7560282961911809_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:13:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7557796597374977_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:13:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7549058766870529_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:13:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7553517890869249_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:13:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7556102157277185_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:13:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7551951518603265_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:13:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7560511805233153_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:13:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7547906653427713_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:13:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7553752415901697_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:13:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7562989890215937_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:13:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7555968501063681_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:13:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7550051743700993_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:13:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7551212345399297_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:13:44 [scrapy.extensions.logstats] INFO: Crawled 79 pages (at 11 pages/min), scraped 5 items (at 0 items/min)
2018-10-30 12:13:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7555659916679169_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:13:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7558011340494849_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:13:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7547856028178433_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:13:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7559143711873025_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:13:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7548416108271617_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:13:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7549096432244737_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:13:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7549949892368385_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:13:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7552947769088001_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:13:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7561017597401089_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:13:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7557981756497921_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:13:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7560566362150913_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:14:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7561855622518785_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:14:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7552136331696129_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:14:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7553607723948033_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:14:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7561407467921409_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:14:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7559854117920769_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:14:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7551179885680641_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:14:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7547686421010433_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:14:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7554785489754113_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:14:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7553451469871105_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:14:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/a/hot/7555804837746689_1.html?type=new> (referer: https://weibo.com/?category=novelty)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 12:14:31 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 12:14:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 5296837,
 'downloader/response_count': 83,
 'downloader/response_status_count/200': 83,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 4, 14, 31, 812179),
 'item_scraped_count': 5,
 'log_count/ERROR': 61,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 83,
 'scheduler/dequeued': 83,
 'scheduler/dequeued/memory': 83,
 'scheduler/enqueued': 83,
 'scheduler/enqueued/memory': 83,
 'spider_exceptions/AttributeError': 61,
 'start_time': datetime.datetime(2018, 10, 30, 3, 56, 43, 433071)}
2018-10-30 12:14:31 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 14:14:59 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-30 14:14:59 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 14:14:59 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-30 14:14:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 14:15:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 14:15:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 14:15:01 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-30 14:15:01 [scrapy.core.engine] INFO: Spider opened
2018-10-30 14:15:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 14:15:02 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-30 14:17:47 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 14 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 14:19:59 [scrapy.extensions.logstats] INFO: Crawled 25 pages (at 11 pages/min), scraped 1 items (at 1 items/min)
2018-10-30 14:19:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300809859898222> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 14:19:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300807674666948> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 14:19:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300774820682860> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 14:19:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300814939223196> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 14:20:22 [scrapy.extensions.logstats] INFO: Crawled 27 pages (at 2 pages/min), scraped 6 items (at 5 items/min)
2018-10-30 14:22:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300807167206955> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 14:22:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300808098305303> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 14:22:27 [scrapy.extensions.logstats] INFO: Crawled 36 pages (at 9 pages/min), scraped 12 items (at 6 items/min)
2018-10-30 14:22:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300744017742894> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 14:22:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300780277465159> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 14:23:09 [scrapy.extensions.logstats] INFO: Crawled 39 pages (at 3 pages/min), scraped 18 items (at 6 items/min)
2018-10-30 14:24:02 [scrapy.extensions.logstats] INFO: Crawled 42 pages (at 3 pages/min), scraped 18 items (at 0 items/min)
2018-10-30 14:24:15 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 14:24:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 1677547,
 'downloader/response_count': 43,
 'downloader/response_status_count/200': 43,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 6, 24, 15, 887403),
 'item_scraped_count': 18,
 'log_count/ERROR': 8,
 'log_count/INFO': 13,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 43,
 'scheduler/dequeued': 43,
 'scheduler/dequeued/memory': 43,
 'scheduler/enqueued': 43,
 'scheduler/enqueued/memory': 43,
 'spider_exceptions/AttributeError': 8,
 'start_time': datetime.datetime(2018, 10, 30, 6, 15, 2, 113729)}
2018-10-30 14:24:15 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 15:26:35 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-30 15:26:35 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 15:26:35 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-30 15:26:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 15:26:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 15:26:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 15:26:38 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-30 15:26:38 [scrapy.core.engine] INFO: Spider opened
2018-10-30 15:26:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 15:26:39 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-30 15:27:50 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 15:28:41 [scrapy.extensions.logstats] INFO: Crawled 7 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 15:29:55 [scrapy.extensions.logstats] INFO: Crawled 11 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 15:30:41 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 15:31:44 [scrapy.extensions.logstats] INFO: Crawled 17 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 15:31:44 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 15:31:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 363059,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 7, 31, 44, 257956),
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'response_received_count': 17,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2018, 10, 30, 7, 26, 39, 36498)}
2018-10-30 15:31:44 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 15:33:28 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-30 15:33:28 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 15:33:28 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-30 15:33:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 15:33:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 15:33:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 15:33:28 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-30 15:33:28 [scrapy.core.engine] INFO: Spider opened
2018-10-30 15:33:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 15:33:28 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-30 15:34:43 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 15:35:51 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 15:36:43 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 15:37:39 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 15:38:35 [scrapy.extensions.logstats] INFO: Crawled 16 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 15:38:52 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 15:38:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 348553,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 7, 38, 52, 432446),
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'response_received_count': 17,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2018, 10, 30, 7, 33, 28, 813936)}
2018-10-30 15:38:52 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 15:41:05 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-30 15:41:05 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 15:41:05 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-30 15:41:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 15:41:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 15:41:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 15:41:06 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-30 15:41:06 [scrapy.core.engine] INFO: Spider opened
2018-10-30 15:41:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 15:41:06 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-30 15:42:19 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 15:43:42 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 15:44:19 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 15:45:12 [scrapy.extensions.logstats] INFO: Crawled 11 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 15:46:11 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 15:47:04 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 15:47:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 353311,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 7, 47, 4, 727604),
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'response_received_count': 17,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2018, 10, 30, 7, 41, 6, 369107)}
2018-10-30 15:47:04 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 19:48:29 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-30 19:48:29 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 19:48:29 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-30 19:48:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 19:48:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 19:48:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 19:48:31 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-30 19:48:31 [scrapy.core.engine] INFO: Spider opened
2018-10-30 19:48:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 19:48:31 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-30 19:51:06 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 14 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 19:53:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300897109828287> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 19:53:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300890298288953> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 19:53:07 [scrapy.extensions.logstats] INFO: Crawled 25 pages (at 11 pages/min), scraped 7 items (at 7 items/min)
2018-10-30 19:54:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300876087952008> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 19:54:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300406795727332> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 19:54:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300872065665742> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 19:54:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300871025456226> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 19:54:58 [scrapy.extensions.logstats] INFO: Crawled 35 pages (at 10 pages/min), scraped 12 items (at 5 items/min)
2018-10-30 19:55:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309351000884300891393002067> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 19:55:34 [scrapy.extensions.logstats] INFO: Crawled 38 pages (at 3 pages/min), scraped 19 items (at 7 items/min)
2018-10-30 19:56:31 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 19:56:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 1847638,
 'downloader/response_count': 43,
 'downloader/response_status_count/200': 43,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 11, 56, 31, 256750),
 'item_scraped_count': 19,
 'log_count/ERROR': 7,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 43,
 'scheduler/dequeued': 43,
 'scheduler/dequeued/memory': 43,
 'scheduler/enqueued': 43,
 'scheduler/enqueued/memory': 43,
 'spider_exceptions/AttributeError': 7,
 'start_time': datetime.datetime(2018, 10, 30, 11, 48, 31, 536399)}
2018-10-30 19:56:31 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 20:13:09 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-30 20:13:09 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 20:13:09 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-30 20:13:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 20:13:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 20:13:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 20:13:09 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-30 20:13:09 [scrapy.core.engine] INFO: Spider opened
2018-10-30 20:13:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 20:13:10 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-30 20:16:56 [scrapy.extensions.logstats] INFO: Crawled 12 pages (at 12 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 20:17:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300906412821509> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 20:17:35 [scrapy.extensions.logstats] INFO: Crawled 15 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 20:17:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300884279477226> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 20:17:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300893817275481> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 20:17:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300903741000950> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 20:17:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300524471108660> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 20:17:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300906437979970> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 20:17:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300907675262197> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 20:17:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300902575021016> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 20:18:21 [scrapy.extensions.logstats] INFO: Crawled 18 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 20:19:23 [scrapy.extensions.logstats] INFO: Crawled 22 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 20:20:12 [scrapy.extensions.logstats] INFO: Crawled 25 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 20:20:12 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 20:20:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 502373,
 'downloader/response_count': 25,
 'downloader/response_status_count/200': 25,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 12, 20, 12, 989786),
 'log_count/ERROR': 8,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 25,
 'scheduler/dequeued': 25,
 'scheduler/dequeued/memory': 25,
 'scheduler/enqueued': 25,
 'scheduler/enqueued/memory': 25,
 'spider_exceptions/AttributeError': 8,
 'start_time': datetime.datetime(2018, 10, 30, 12, 13, 9, 998592)}
2018-10-30 20:20:12 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 20:23:41 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-30 20:23:41 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 20:23:41 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-30 20:23:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 20:23:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 20:23:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 20:23:41 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-30 20:23:41 [scrapy.core.engine] INFO: Spider opened
2018-10-30 20:23:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 20:23:41 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-30 20:24:43 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 20:25:51 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 20:26:53 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 20:27:36 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 20:27:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 332943,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 12, 27, 36, 640161),
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'response_received_count': 17,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2018, 10, 30, 12, 23, 41, 837731)}
2018-10-30 20:27:36 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 20:28:54 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-30 20:28:54 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 20:28:54 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-30 20:28:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 20:28:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 20:28:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 20:28:54 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-30 20:28:54 [scrapy.core.engine] INFO: Spider opened
2018-10-30 20:28:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 20:28:54 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-30 20:48:07 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-30 20:48:07 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 20:48:07 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-30 20:48:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 20:48:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 20:48:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 20:48:07 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-30 20:48:07 [scrapy.core.engine] INFO: Spider opened
2018-10-30 20:48:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 20:48:07 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-30 20:50:43 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 13 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 20:51:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300914562334313> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 20:52:58 [scrapy.extensions.logstats] INFO: Crawled 24 pages (at 11 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 20:52:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300890977793710> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 20:52:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300532188631207> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 20:52:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300915166283239> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 20:53:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300913245342292> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 20:53:23 [scrapy.extensions.logstats] INFO: Crawled 26 pages (at 2 pages/min), scraped 4 items (at 4 items/min)
2018-10-30 20:55:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404232277776317869> (referer: https://weibo.com/?category=10007)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 20:55:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300906714767205> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 20:55:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300905930445961> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 20:55:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300905238365370> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 20:55:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300912658149091> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 20:55:15 [scrapy.extensions.logstats] INFO: Crawled 35 pages (at 9 pages/min), scraped 7 items (at 3 items/min)
2018-10-30 20:55:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300857813419394> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 20:55:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300886875750492> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 20:55:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300905418737574> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 59, in parse_item
    time_list = response.xpath(r'//span[@class="time"]/text()').extract_first().split(' ')
AttributeError: 'NoneType' object has no attribute 'split'
2018-10-30 20:56:13 [scrapy.extensions.logstats] INFO: Crawled 40 pages (at 5 pages/min), scraped 12 items (at 5 items/min)
2018-10-30 20:56:37 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 20:56:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 1458241,
 'downloader/response_count': 42,
 'downloader/response_status_count/200': 42,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 12, 56, 37, 786749),
 'item_scraped_count': 12,
 'log_count/ERROR': 13,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 42,
 'scheduler/dequeued': 42,
 'scheduler/dequeued/memory': 42,
 'scheduler/enqueued': 42,
 'scheduler/enqueued/memory': 42,
 'spider_exceptions/AttributeError': 13,
 'start_time': datetime.datetime(2018, 10, 30, 12, 48, 7, 857583)}
2018-10-30 20:56:37 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 21:05:29 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-30 21:05:29 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 21:05:29 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-30 21:05:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 21:05:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 21:05:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 21:05:29 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-30 21:05:29 [scrapy.core.engine] INFO: Spider opened
2018-10-30 21:05:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 21:05:29 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-30 21:08:13 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 13 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 21:08:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300917968111121> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 54, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:08:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300915166283239> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 54, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:08:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300914562334313> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 54, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:08:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300916852398968> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 54, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:08:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300919192860351> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 54, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:08:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300917741598078> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 54, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:08:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300916730821218> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 54, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:08:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300916491726204> (referer: https://weibo.com/?category=12)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 54, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:08:35 [scrapy.extensions.logstats] INFO: Crawled 15 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 21:09:36 [scrapy.extensions.logstats] INFO: Crawled 20 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 21:10:37 [scrapy.extensions.logstats] INFO: Crawled 25 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 21:10:37 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 21:10:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 948487,
 'downloader/response_count': 25,
 'downloader/response_status_count/200': 25,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 13, 10, 37, 908801),
 'log_count/ERROR': 8,
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 25,
 'scheduler/dequeued': 25,
 'scheduler/dequeued/memory': 25,
 'scheduler/enqueued': 25,
 'scheduler/enqueued/memory': 25,
 'spider_exceptions/AttributeError': 8,
 'start_time': datetime.datetime(2018, 10, 30, 13, 5, 29, 752176)}
2018-10-30 21:10:37 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 21:11:59 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-30 21:11:59 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 21:11:59 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-30 21:11:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 21:11:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 21:11:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 21:11:59 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-30 21:11:59 [scrapy.core.engine] INFO: Spider opened
2018-10-30 21:11:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 21:11:59 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\core\engine.py", line 127, in _next_request
    request = next(slot.start_requests)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in start_requests
    if method_is_overridden(cls, self.Spider, 'make_requests_from_url'):
AttributeError: 'TextSpider' object has no attribute 'Spider'
2018-10-30 21:11:59 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 21:11:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 13, 11, 59, 822487),
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'start_time': datetime.datetime(2018, 10, 30, 13, 11, 59, 795485)}
2018-10-30 21:11:59 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 21:12:06 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-30 21:12:06 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 21:12:06 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-30 21:12:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 21:12:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 21:12:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 21:12:07 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-30 21:12:07 [scrapy.core.engine] INFO: Spider opened
2018-10-30 21:12:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 21:12:07 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\core\engine.py", line 127, in _next_request
    request = next(slot.start_requests)
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 56, in start_requests
    if method_is_overridden(cls, self.Spider, 'make_requests_from_url'):
AttributeError: 'TextSpider' object has no attribute 'Spider'
2018-10-30 21:12:07 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 21:12:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 13, 12, 7, 578930),
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'start_time': datetime.datetime(2018, 10, 30, 13, 12, 7, 559929)}
2018-10-30 21:12:07 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 21:13:54 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-30 21:13:54 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 21:13:54 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-30 21:13:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 21:13:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 21:13:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 21:13:55 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-30 21:13:55 [scrapy.core.engine] INFO: Spider opened
2018-10-30 21:13:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 21:13:55 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-30 21:14:55 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 21:17:30 [scrapy.extensions.logstats] INFO: Crawled 17 pages (at 12 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 21:17:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300918052003219> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:18:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300903594253556> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:18:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300915979979578> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:18:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300912658149091> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:18:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300913245342292> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:18:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300918630803692> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:18:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309614300916361691654> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:18:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300921147410042> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:19:31 [scrapy.extensions.logstats] INFO: Crawled 26 pages (at 9 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 21:19:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300914927255612> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:19:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309351002454300848418160523> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:19:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300897600589171> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:19:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300905418737574> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:19:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300904500169118> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:19:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300857813419394> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:19:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300901660650215> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:19:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300886875750492> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:19:58 [scrapy.extensions.logstats] INFO: Crawled 28 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 21:21:05 [scrapy.extensions.logstats] INFO: Crawled 33 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 21:21:05 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 21:21:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 959255,
 'downloader/response_count': 33,
 'downloader/response_status_count/200': 33,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 13, 21, 5, 366690),
 'log_count/ERROR': 16,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 33,
 'scheduler/dequeued': 33,
 'scheduler/dequeued/memory': 33,
 'scheduler/enqueued': 33,
 'scheduler/enqueued/memory': 33,
 'spider_exceptions/AttributeError': 16,
 'start_time': datetime.datetime(2018, 10, 30, 13, 13, 55, 316092)}
2018-10-30 21:21:05 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 21:34:31 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-30 21:34:31 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 21:34:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-30 21:34:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 21:34:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 21:34:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 21:34:32 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-30 21:34:32 [scrapy.core.engine] INFO: Spider opened
2018-10-30 21:34:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 21:34:32 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-30 21:36:06 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 8 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 21:37:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2310474300916735015866> (referer: https://weibo.com/?category=7)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:38:22 [scrapy.extensions.logstats] INFO: Crawled 19 pages (at 11 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 21:38:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300931075339603> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:39:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404232016534098055> (referer: https://weibo.com/?category=10007)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:39:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309614300916361691654> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:39:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300921147410042> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:39:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300918052003219> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:39:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300880492006401> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:39:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300927451456594> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:39:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300929334694224> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:39:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300919117357453> (referer: https://weibo.com/?category=10018)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:39:59 [scrapy.extensions.logstats] INFO: Crawled 26 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 21:40:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300910242215356> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:40:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300857813419394> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:40:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300914927255612> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:40:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300858497068503> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:40:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300911198478096> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:40:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300918433655992> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:40:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300852859915197> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:40:41 [scrapy.extensions.logstats] INFO: Crawled 29 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 21:40:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://weibo.com/ttarticle/p/show?id=2309404300904500169118> (referer: https://weibo.com/?category=1760)
Traceback (most recent call last):
  File "C:\Python 3.6\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Python 3.6\lib\site-packages\scrapy\spiders\crawl.py", line 78, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "C:\Users\Administrator\PycharmProjects\spider_weibo\weibo\weibo\spiders\text.py", line 74, in parse_item
    print('1',response.cookies().headers)
AttributeError: 'HtmlResponse' object has no attribute 'cookies'
2018-10-30 21:41:43 [scrapy.extensions.logstats] INFO: Crawled 33 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 21:42:08 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 21:42:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 1060021,
 'downloader/response_count': 35,
 'downloader/response_status_count/200': 35,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 13, 42, 8, 210920),
 'log_count/ERROR': 18,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'request_depth_max': 1,
 'response_received_count': 35,
 'scheduler/dequeued': 35,
 'scheduler/dequeued/memory': 35,
 'scheduler/enqueued': 35,
 'scheduler/enqueued/memory': 35,
 'spider_exceptions/AttributeError': 18,
 'start_time': datetime.datetime(2018, 10, 30, 13, 34, 32, 107833)}
2018-10-30 21:42:08 [scrapy.core.engine] INFO: Spider closed (finished)
2018-10-30 21:44:08 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: weibo)
2018-10-30 21:44:08 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1
2018-10-30 21:44:08 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'weibo', 'LOG_FILE': 'TencentSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'weibo.spiders', 'SPIDER_MODULES': ['weibo.spiders']}
2018-10-30 21:44:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2018-10-30 21:44:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'weibo.middlewares.middleware.JavaScriptMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-10-30 21:44:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-10-30 21:44:08 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2018-10-30 21:44:08 [scrapy.core.engine] INFO: Spider opened
2018-10-30 21:44:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 21:44:08 [py.warnings] WARNING: C:\Python 3.6\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '

2018-10-30 21:45:10 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 21:46:11 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 21:47:10 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 21:48:16 [scrapy.extensions.logstats] INFO: Crawled 17 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2018-10-30 21:48:16 [scrapy.core.engine] INFO: Closing spider (finished)
2018-10-30 21:48:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 333186,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 10, 30, 13, 48, 16, 482984),
 'log_count/INFO': 11,
 'log_count/WARNING': 1,
 'response_received_count': 17,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2018, 10, 30, 13, 44, 8, 950826)}
2018-10-30 21:48:16 [scrapy.core.engine] INFO: Spider closed (finished)
